{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../categories.txt\", \"r\") as f:\n",
    "    classes = [line.strip().replace(' ', '_') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_quickdraw_data(root, vfold_ratio=0.1, max_items_per_class=50000, img_size=(28, 28)):\n",
    "    \"\"\"\n",
    "    Load QuickDraw data from .npy files, process images, and split into train/val sets.\n",
    "\n",
    "    Parameters:\n",
    "        root (str): Path to the directory containing QuickDraw .npy files.\n",
    "        vfold_ratio (float): Fraction of the data to use as validation.\n",
    "        max_items_per_class (int): Maximum number of items to load per class.\n",
    "        img_size (tuple): Size to reshape images (28, 28 for QuickDraw).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor): Train and validation images and labels.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "    num_classes = len(all_files)\n",
    "    \n",
    "    # Pre-allocate arrays based on max_items_per_class\n",
    "    x = np.empty((num_classes * max_items_per_class, img_size[0] * img_size[1]), dtype=np.uint8)\n",
    "    y = np.empty((num_classes * max_items_per_class,), dtype=np.int64)\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # Load and process each class file\n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file, mmap_mode='r')[:max_items_per_class, :].astype(np.uint8)\n",
    "        num_samples = data.shape[0]\n",
    "\n",
    "        # Place data in the pre-allocated arrays\n",
    "        x[current_index:current_index + num_samples] = data\n",
    "        y[current_index:current_index + num_samples] = idx\n",
    "        current_index += num_samples\n",
    "\n",
    "    # Trim x and y to the actual number of samples loaded\n",
    "    x = x[:current_index]\n",
    "    y = y[:current_index]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(len(y))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Reshape images and normalize\n",
    "    x = x.reshape(-1, *img_size)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=vfold_ratio, random_state=42)\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28)  # Reshape to original 2D image\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL for torchvision transforms\n",
    "        image = Image.fromarray(image, mode=\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# Apply transforms for resizing and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Assume data is loaded from a .npy file or similar format\n",
    "X_train, X_val, y_train, y_val = load_quickdraw_data('../data', vfold_ratio=0.1, max_items_per_class=1000)\n",
    "\n",
    "train_dataset = QuickDrawDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = QuickDrawDataset(X_val, y_val, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ink2DoodleNet(nn.Module):\n",
    "    def __init__(self, num_classes=345):\n",
    "        super(Ink2DoodleNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64),nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64),nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*3*3, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_and_save_ink2doodle_weights(checkpoint_path, model_class, num_classes, lr=0.001, save_path=\"ink2doodle_converted_weights.pth\"):\n",
    "#     # Initialize the base model class\n",
    "#     model = model_class(num_classes=num_classes)\n",
    "    \n",
    "#     # Load checkpoint into memory\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "\n",
    "#     # Strip \"model.\" prefix from the keys in the state_dict\n",
    "#     adjusted_state_dict = {k.replace(\"model.\", \"\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "    \n",
    "#     # Load adjusted state_dict into the model\n",
    "#     model.load_state_dict(adjusted_state_dict, strict=True)\n",
    "    \n",
    "#     # Save the adjusted state dict\n",
    "#     torch.save(model.state_dict(), save_path)\n",
    "#     print(f\"Converted weights saved to {save_path}\")\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Load, adjust, and save the checkpoint for Ink2DoodleNet\n",
    "# checkpoint_path = r\"D:\\masters-ml-projects\\Fundamentals of Computer Vision\\DoodleClassifier\\ink2doodle\\models\\ink2doodle.ckpt\"\n",
    "# save_path = r\"D:\\masters-ml-projects\\Fundamentals of Computer Vision\\DoodleClassifier\\ink2doodle\\models\\ink2doodle_converted_weights.pth\"\n",
    "# model = load_and_save_ink2doodle_weights(checkpoint_path, Ink2DoodleNet, num_classes=345, save_path=save_path)\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balsr\\AppData\\Local\\Temp\\ipykernel_36060\\291298645.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ink2DoodleNet(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0.1, inplace=False)\n",
       "    (16): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU()\n",
       "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (23): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=576, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=345, bias=True)\n",
       "    (5): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_ink2doodle_weights(checkpoint_path, model_class, num_classes, lr=0.001):\n",
    "    # Initialize the base model class\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    \n",
    "    # Load checkpoint into memory\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Load adjusted state_dict into the model\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the checkpoint into Ink2DoodleNet\n",
    "checkpoint_path = r\"D:\\masters-ml-projects\\Fundamentals of Computer Vision\\DoodleClassifier\\ink2doodle\\models\\ink2doodle.pth\"\n",
    "model = load_ink2doodle_weights(checkpoint_path, Ink2DoodleNet, num_classes=345)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Predictions:\n",
      "1: coffee_cup (78.17%)\n",
      "2: cake (9.10%)\n",
      "3: birthday_cake (7.56%)\n",
      "4: hot_tub (1.63%)\n",
      "5: mug (1.61%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Display an example prediction\n",
    "idx = randint(0, len(val_loader.dataset))\n",
    "img, true_label = val_loader.dataset[idx]\n",
    "img = img.unsqueeze(0).to(device)  # Add batch dimension and move to the model's device\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(img.cpu().squeeze(), cmap='gray')\n",
    "plt.title(f\"True Label: {classes[true_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "    probabilities = torch.softmax(output, dim=1)[0]\n",
    "    top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Display the top 5 predictions with probabilities\n",
    "print(\"Top 5 Predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}: {classes[top_5_indices[i].item()]} ({top_5_probs[i].item() * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy on subset of 1000 samples: 88.60%\n"
     ]
    }
   ],
   "source": [
    "def top_k_accuracy(model, data_loader, k=5, subset_size=100):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for evaluation\n",
    "        for idx, (images, labels) in enumerate(data_loader):\n",
    "            if total >= subset_size:\n",
    "                break  # Stop after reaching the subset size\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, top_k_preds = torch.topk(outputs, k, dim=1)  # Get the top k predictions\n",
    "            top_k_preds = top_k_preds.t()  # Transpose for easy comparison\n",
    "            correct += (top_k_preds == labels.view(1, -1)).any(dim=0).sum().item()  # Check if any top k is correct\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Stop early if adding this batch exceeds subset size\n",
    "            if total > subset_size:\n",
    "                correct -= (top_k_preds == labels.view(1, -1)).any(dim=0).sum().item()\n",
    "                total -= labels.size(0)\n",
    "                remaining = subset_size - total\n",
    "                correct += (top_k_preds[:, :remaining] == labels.view(1, -1)[:, :remaining]).any(dim=0).sum().item()\n",
    "                total += remaining\n",
    "                break\n",
    "\n",
    "    # Calculate accuracy as a percentage\n",
    "    top_k_acc = (correct / total) * 100\n",
    "    return top_k_acc\n",
    "\n",
    "subset_size=1000\n",
    "# Example usage with k=5 and subset_size=100\n",
    "top_k_acc = top_k_accuracy(model, val_loader, k=5, subset_size=subset_size)\n",
    "print(f\"Top-5 Accuracy on subset of {subset_size} samples: {top_k_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
