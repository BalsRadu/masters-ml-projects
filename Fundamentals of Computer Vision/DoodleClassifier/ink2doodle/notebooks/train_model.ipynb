{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"categories.txt\", \"r\") as f:\n",
    "    classes = [line.strip().replace(' ', '_') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_quickdraw_data(root, vfold_ratio=0.1, max_items_per_class=50000, img_size=(28, 28)):\n",
    "    \"\"\"\n",
    "    Load QuickDraw data from .npy files, process images, and split into train/val sets.\n",
    "\n",
    "    Parameters:\n",
    "        root (str): Path to the directory containing QuickDraw .npy files.\n",
    "        vfold_ratio (float): Fraction of the data to use as validation.\n",
    "        max_items_per_class (int): Maximum number of items to load per class.\n",
    "        img_size (tuple): Size to reshape images (28, 28 for QuickDraw).\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor): Train and validation images and labels.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "    num_classes = len(all_files)\n",
    "    \n",
    "    # Pre-allocate arrays based on max_items_per_class\n",
    "    x = np.empty((num_classes * max_items_per_class, img_size[0] * img_size[1]), dtype=np.uint8)\n",
    "    y = np.empty((num_classes * max_items_per_class,), dtype=np.int64)\n",
    "    \n",
    "    current_index = 0\n",
    "\n",
    "    # Load and process each class file\n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file, mmap_mode='r')[:max_items_per_class, :].astype(np.uint8)\n",
    "        num_samples = data.shape[0]\n",
    "\n",
    "        # Place data in the pre-allocated arrays\n",
    "        x[current_index:current_index + num_samples] = data\n",
    "        y[current_index:current_index + num_samples] = idx\n",
    "        current_index += num_samples\n",
    "\n",
    "    # Trim x and y to the actual number of samples loaded\n",
    "    x = x[:current_index]\n",
    "    y = y[:current_index]\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(len(y))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Reshape images and normalize\n",
    "    x = x.reshape(-1, *img_size)\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=vfold_ratio, random_state=42)\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class QuickDrawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28, 28)  # Reshape to original 2D image\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL for torchvision transforms\n",
    "        image = Image.fromarray(image, mode=\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# Apply transforms for resizing and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Assume data is loaded from a .npy file or similar format\n",
    "X_train, X_val, y_train, y_val = load_quickdraw_data('../data', vfold_ratio=0.1, max_items_per_class=50000)\n",
    "\n",
    "train_dataset = QuickDrawDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = QuickDrawDataset(X_val, y_val, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ink2DoodleNet(nn.Module):\n",
    "    def __init__(self, num_classes=345):\n",
    "        super(Ink2DoodleNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64),nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64),nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*3*3, 512), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: d:\\masters-ml-projects\\Fundamentals of Computer Vision\\DoodleClassifier\\ink2doodle\\notebooks\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type               | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model           | Ink2DoodleNet      | 544 K  | train\n",
      "1 | criterion       | CrossEntropyLoss   | 0      | train\n",
      "2 | train_top_k_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_top_k_acc   | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------------\n",
      "544 K     Trainable params\n",
      "0         Non-trainable params\n",
      "544 K     Total params\n",
      "2.179     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|â–Œ         | 3172/60645 [01:46<32:15, 29.69it/s, v_num=0, train_top_k_acc=0.883, train_loss=1.430, val_top_k_acc=0.893, val_loss=1.260] "
     ]
    }
   ],
   "source": [
    "import torchmetrics  # Import torchmetrics for accuracy metrics\n",
    "\n",
    "class Ink2DoodleWrapper(pl.LightningModule):\n",
    "    def __init__(self, num_classes, lr=0.001, k=5):\n",
    "        super(Ink2DoodleWrapper, self).__init__()\n",
    "        # Initialize Ink2DoodleNet with specified number of classes\n",
    "        self.model = Ink2DoodleNet(num_classes=num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "        self.k = k  # Top-k value\n",
    "        # Define metrics\n",
    "        self.train_top_k_acc = torchmetrics.Accuracy(top_k=self.k, task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_top_k_acc = torchmetrics.Accuracy(top_k=self.k, task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate and log top-k accuracy for training\n",
    "        top_k_acc = self.train_top_k_acc(outputs, labels)\n",
    "        self.log(\"train_top_k_acc\", top_k_acc, prog_bar=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        # Calculate and log top-k accuracy for validation\n",
    "        top_k_acc = self.val_top_k_acc(outputs, labels)\n",
    "        self.log(\"val_top_k_acc\", top_k_acc, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.1)  # Adjust the learning rate every 1 epoch\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_top_k_acc\"}\n",
    "\n",
    "# Set up DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Define callbacks for checkpointing and early stopping\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_top_k_acc\",\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"ink2doodle-{epoch:02d}-{val_top_k_acc:.2f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"max\",\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_top_k_acc\", patience=3, mode=\"max\")\n",
    "\n",
    "# Initialize the PyTorch Lightning model and trainer with callbacks\n",
    "model = Ink2DoodleWrapper(num_classes=345)\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balsr\\AppData\\Local\\Temp\\ipykernel_361072\\3576567677.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ink2DoodleWrapper(\n",
       "  (model): Ink2DoodleNet(\n",
       "    (conv_layers): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Dropout(p=0.1, inplace=False)\n",
       "      (8): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU()\n",
       "      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (15): Dropout(p=0.1, inplace=False)\n",
       "      (16): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (18): ReLU()\n",
       "      (19): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (21): ReLU()\n",
       "      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (23): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (fc_layers): Sequential(\n",
       "      (0): Flatten(start_dim=1, end_dim=-1)\n",
       "      (1): Linear(in_features=576, out_features=512, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "      (4): Linear(in_features=512, out_features=345, bias=True)\n",
       "      (5): LogSoftmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (train_top_k_acc): MulticlassAccuracy()\n",
       "  (val_top_k_acc): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model_class, num_classes, lr=0.001):\n",
    "    model = model_class(num_classes=num_classes, lr=lr)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu')) \n",
    "    model.load_state_dict(checkpoint['state_dict'])  \n",
    "\n",
    "    return model\n",
    "\n",
    "checkpoint_path = r\"D:\\masters-ml-projects\\Fundamentals of Computer Vision\\DoodleClassifier\\ink2doodle\\models\\ink2doodle.ckpt\"\n",
    "loaded_model = load_checkpoint(checkpoint_path, Ink2DoodleWrapper, num_classes=345)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Predictions:\n",
      "1: kangaroo (93.24%)\n",
      "2: bear (0.90%)\n",
      "3: monkey (0.78%)\n",
      "4: rabbit (0.74%)\n",
      "5: cat (0.69%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Display an example prediction\n",
    "idx = randint(0, len(val_loader.dataset))\n",
    "img, true_label = val_loader.dataset[idx]\n",
    "img = img.unsqueeze(0).to(device)  # Add batch dimension and move to the model's device\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(img.cpu().squeeze(), cmap='gray')\n",
    "plt.title(f\"True Label: {classes[true_label]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "    probabilities = torch.softmax(output, dim=1)[0]\n",
    "    top_5_probs, top_5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Display the top 5 predictions with probabilities\n",
    "print(\"Top 5 Predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}: {classes[top_5_indices[i].item()]} ({top_5_probs[i].item() * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy on subset of 50000 samples: 89.36%\n"
     ]
    }
   ],
   "source": [
    "def top_k_accuracy(model, data_loader, k=5, subset_size=100):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for evaluation\n",
    "        for idx, (images, labels) in enumerate(data_loader):\n",
    "            if total >= subset_size:\n",
    "                break  # Stop after reaching the subset size\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, top_k_preds = torch.topk(outputs, k, dim=1)  # Get the top k predictions\n",
    "            top_k_preds = top_k_preds.t()  # Transpose for easy comparison\n",
    "            correct += (top_k_preds == labels.view(1, -1)).any(dim=0).sum().item()  # Check if any top k is correct\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Stop early if adding this batch exceeds subset size\n",
    "            if total > subset_size:\n",
    "                correct -= (top_k_preds == labels.view(1, -1)).any(dim=0).sum().item()\n",
    "                total -= labels.size(0)\n",
    "                remaining = subset_size - total\n",
    "                correct += (top_k_preds[:, :remaining] == labels.view(1, -1)[:, :remaining]).any(dim=0).sum().item()\n",
    "                total += remaining\n",
    "                break\n",
    "\n",
    "    # Calculate accuracy as a percentage\n",
    "    top_k_acc = (correct / total) * 100\n",
    "    return top_k_acc\n",
    "\n",
    "subset_size=50000\n",
    "# Example usage with k=5 and subset_size=100\n",
    "top_k_acc = top_k_accuracy(model, val_loader, k=5, subset_size=subset_size)\n",
    "print(f\"Top-5 Accuracy on subset of {subset_size} samples: {top_k_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
